# ml/train_improved_models.py
"""
Enhanced Training Pipeline for Health Prediction Models - Version 3

KEY IMPROVEMENTS:
1. Stronger regularization (reduced overfitting from 80% to target <15%)
2. Increased lookback window to 28 days (from 14)
3. 50% data sampling (from 30%) for better patterns
4. Realistic correlations in synthetic data
5. Better feature engineering with lag features
6. Temporal cross-validation option
7. Comprehensive monitoring and reporting
"""

import numpy as np
import pandas as pd
from pathlib import Path
import sys
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from backend.ml.data.synthetic_generator import save_synthetic_data
from backend.ml.data.feature_engineering_v2 import create_ml_features
from backend.ml.models.health_predictor_v2 import ImprovedHealthPredictor


def evaluate_regression_model(y_true, y_pred, model_name):
    """Comprehensive evaluation for regression models"""
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # Additional metrics
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-6))) * 100
    explained_var = explained_variance_score(y_true, y_pred)
    
    # Median absolute error (more robust to outliers)
    median_ae = np.median(np.abs(y_true - y_pred))
    
    print(f"\n{model_name} Evaluation:")
    print(f"  MSE:                {mse:.4f}")
    print(f"  RMSE:               {rmse:.4f}")
    print(f"  MAE:                {mae:.4f}")
    print(f"  Median AE:          {median_ae:.4f}")
    print(f"  RÂ² Score:           {r2:.4f}")
    print(f"  MAPE:               {mape:.2f}%")
    print(f"  Explained Variance: {explained_var:.4f}")
    
    return {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'median_ae': median_ae,
        'r2_score': r2,
        'mape': mape,
        'explained_variance': explained_var
    }


def evaluate_classification_model(y_true, y_pred_proba, model_name, threshold=0.5):
    """Comprehensive evaluation for classification models"""
    y_pred = (y_pred_proba >= threshold).astype(int)
    
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    auc_roc = roc_auc_score(y_true, y_pred_proba)
    
    print(f"\n{model_name} Evaluation:")
    print(f"  Accuracy:  {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1 Score:  {f1:.4f}")
    print(f"  AUC-ROC:   {auc_roc:.4f}")
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'auc_roc': auc_roc
    }


def plot_predictions(y_true, y_pred, model_name, save_path):
    """Plot predicted vs actual values with residuals"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Scatter plot
    axes[0].scatter(y_true, y_pred, alpha=0.5, s=10, color='steelblue')
    axes[0].plot([y_true.min(), y_true.max()], 
                 [y_true.min(), y_true.max()], 
                 'r--', lw=2, label='Perfect Prediction')
    axes[0].set_xlabel('Actual Values', fontsize=12)
    axes[0].set_ylabel('Predicted Values', fontsize=12)
    axes[0].set_title(f'{model_name}: Predicted vs Actual', fontsize=14, fontweight='bold')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Add RÂ² score to plot
    r2 = r2_score(y_true, y_pred)
    axes[0].text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=axes[0].transAxes,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    # Residual plot
    residuals = y_true - y_pred
    axes[1].scatter(y_pred, residuals, alpha=0.5, s=10, color='coral')
    axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
    axes[1].set_xlabel('Predicted Values', fontsize=12)
    axes[1].set_ylabel('Residuals', fontsize=12)
    axes[1].set_title(f'{model_name}: Residual Plot', fontsize=14, fontweight='bold')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  Saved plot to {save_path}")


def plot_feature_importance(importance_dict, model_name, save_path, top_n=15):
    """Plot feature importance"""
    if not importance_dict:
        return
    
    features = list(importance_dict.keys())[:top_n]
    importances = list(importance_dict.values())[:top_n]
    
    plt.figure(figsize=(10, 8))
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(features)))
    plt.barh(range(len(features)), importances, color=colors)
    plt.yticks(range(len(features)), features, fontsize=10)
    plt.xlabel('Importance', fontsize=12)
    plt.title(f'{model_name}: Top {top_n} Feature Importances', 
             fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  Saved feature importance to {save_path}")


def plot_training_summary(results, save_path):
    """Create summary visualization of all models"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # RÂ² scores
    regression_targets = ['sleep_quality', 'mood_score', 'stress_level']
    r2_scores = [results[t]['r2_score'] for t in regression_targets]
    axes[0, 0].bar(regression_targets, r2_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
    axes[0, 0].set_ylabel('RÂ² Score', fontsize=12)
    axes[0, 0].set_title('Model Performance (RÂ² Score)', fontsize=14, fontweight='bold')
    axes[0, 0].set_ylim([0, 1])
    axes[0, 0].axhline(y=0.3, color='green', linestyle='--', label='Target (0.3)')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    # RMSE
    rmse_scores = [results[t]['rmse'] for t in regression_targets]
    axes[0, 1].bar(regression_targets, rmse_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
    axes[0, 1].set_ylabel('RMSE', fontsize=12)
    axes[0, 1].set_title('Prediction Error (RMSE)', fontsize=14, fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # Classification metrics
    class_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']
    class_values = [results['exercise_binary'][m] for m in class_metrics]
    axes[1, 0].bar(class_metrics, class_values, color='#95E1D3')
    axes[1, 0].set_ylabel('Score', fontsize=12)
    axes[1, 0].set_title('Exercise Prediction Metrics', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylim([0, 1])
    axes[1, 0].set_xticklabels(class_metrics, rotation=45, ha='right')
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    
    # Comparison table
    axes[1, 1].axis('off')
    table_data = []
    for target in regression_targets:
        table_data.append([
            target.replace('_', ' ').title(),
            f"{results[target]['r2_score']:.3f}",
            f"{results[target]['mae']:.3f}",
            f"{results[target]['rmse']:.3f}"
        ])
    
    table = axes[1, 1].table(cellText=table_data,
                            colLabels=['Metric', 'RÂ²', 'MAE', 'RMSE'],
                            cellLoc='center',
                            loc='center',
                            bbox=[0, 0.2, 1, 0.6])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    
    # Style header
    for i in range(4):
        table[(0, i)].set_facecolor('#40466e')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    axes[1, 1].set_title('Summary Table', fontsize=14, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  Saved training summary to {save_path}")


def main():
    print("="*80)
    print("ENHANCED HEALTH PREDICTION MODEL TRAINING PIPELINE V3")
    print("="*80)
    print("\nImprovements:")
    print("  âœ“ Stronger regularization to reduce overfitting")
    print("  âœ“ 28-day lookback window (from 14)")
    print("  âœ“ 50% data sampling (from 30%)")
    print("  âœ“ Realistic correlations in synthetic data")
    print("  âœ“ Enhanced feature engineering")
    
    # Configuration
    SAMPLE_RATE = 0.5       # Use 50% of users
    LOOKBACK_DAYS = 28      # 28-day lookback window
    MODEL_TYPE = 'stacking' # Ensemble approach
    
    # Create output directories
    results_dir = Path("ml/evaluation_results_v3")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    models_dir = Path("ml/trained_models")
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # Step 1: Generate synthetic data with correlations
    print("\n" + "="*80)
    print("STEP 1: Generating Synthetic Health Data (with correlations)")
    print("="*80)
    
    datasets = save_synthetic_data()
    
    print("\nDataset Statistics:")
    for name, df in datasets.items():
        print(f"  {name:15s}: {len(df):,} records")
    
    # Step 2: Create ML features
    print("\n" + "="*80)
    print("STEP 2: Feature Engineering")
    print("="*80)
    
    ml_features = create_ml_features(
        datasets, 
        sample_rate=SAMPLE_RATE,
        lookback_days=LOOKBACK_DAYS
    )
    
    X = ml_features['X_train']
    y_dict = {
        'sleep_quality': ml_features['y_train'][:, 0],
        'mood_score': ml_features['y_train'][:, 1],
        'stress_level': ml_features['y_train'][:, 2],
        'exercise_binary': ml_features['y_train'][:, 3]
    }
    feature_names = ml_features['feature_names']
    
    print(f"\nFinal Dataset Shape:")
    print(f"  Samples:  {X.shape[0]:,}")
    print(f"  Features: {X.shape[1]}")
    
    # Step 3: Split data
    print("\n" + "="*80)
    print("STEP 3: Train/Validation Split (80/20)")
    print("="*80)
    
    indices = np.arange(len(X))
    train_idx, val_idx = train_test_split(
        indices, 
        test_size=0.2, 
        random_state=42, 
        shuffle=True
    )
    
    X_train, X_val = X[train_idx], X[val_idx]
    y_train = {k: v[train_idx] for k, v in y_dict.items()}
    y_val = {k: v[val_idx] for k, v in y_dict.items()}
    
    print(f"  Training samples:   {len(X_train):,}")
    print(f"  Validation samples: {len(X_val):,}")
    print(f"  Train/Val ratio:    {len(X_train)/len(X_val):.2f}")
    
    # Step 4: Train models
    print("\n" + "="*80)
    print(f"STEP 4: Model Training ({MODEL_TYPE.upper()} with Strong Regularization)")
    print("="*80)
    
    predictor = ImprovedHealthPredictor(model_type=MODEL_TYPE)
    predictor.build_models(X.shape[1])
    predictor.train(X_train, y_train, X_val, y_val)
    
    # Step 5: Evaluation
    print("\n" + "="*80)
    print("STEP 5: Model Evaluation")
    print("="*80)
    
    predictions = predictor.predict(X_val)
    
    results = {}
    
    # Evaluate regression models
    for target in ['sleep_quality', 'mood_score', 'stress_level']:
        print(f"\n{'-'*80}")
        metrics = evaluate_regression_model(
            y_val[target], 
            predictions[target],
            target.replace('_', ' ').title()
        )
        results[target] = metrics
        
        # Plot predictions
        plot_path = results_dir / f"{target}_predictions_v3.png"
        plot_predictions(
            y_val[target],
            predictions[target],
            target.replace('_', ' ').title(),
            plot_path
        )
    
    # Evaluate classification model
    print(f"\n{'-'*80}")
    metrics = evaluate_classification_model(
        y_val['exercise_binary'],
        predictions['exercise_binary'],
        'Exercise Prediction'
    )
    results['exercise_binary'] = metrics
    
    # Step 6: Feature Importance
    print("\n" + "="*80)
    print("STEP 6: Feature Importance Analysis")
    print("="*80)
    
    importance = predictor.get_feature_importance(feature_names, top_n=20)
    
    for target, features in importance.items():
        print(f"\nTop 10 features for {target}:")
        for i, (feat, imp) in enumerate(list(features.items())[:10], 1):
            print(f"  {i:2d}. {feat:50s}: {imp:.4f}")
        
        # Plot feature importance
        plot_path = results_dir / f"{target}_feature_importance_v3.png"
        plot_feature_importance(features, target.replace('_', ' ').title(), plot_path)
    
    # Step 7: Summary visualizations
    print("\n" + "="*80)
    print("STEP 7: Creating Summary Visualizations")
    print("="*80)
    
    summary_path = results_dir / 'training_summary_v3.png'
    plot_training_summary(results, summary_path)
    
    # Step 8: Save models
    print("\n" + "="*80)
    print("STEP 8: Saving Models")
    print("="*80)
    
    predictor.save_models()
    
    # Save feature names
    feature_names_path = models_dir / 'feature_names_v3.txt'
    with open(feature_names_path, 'w') as f:
        for name in feature_names:
            f.write(f"{name}\n")
    print(f"Saved feature names to {feature_names_path}")
    
    # Save evaluation results
    results_path = results_dir / 'evaluation_metrics_v3.txt'
    with open(results_path, 'w') as f:
        f.write("="*80 + "\n")
        f.write("HEALTH PREDICTION MODELS - EVALUATION REPORT V3\n")
        f.write("="*80 + "\n\n")
        f.write(f"Configuration:\n")
        f.write(f"  Model Type: {MODEL_TYPE}\n")
        f.write(f"  Lookback Days: {LOOKBACK_DAYS}\n")
        f.write(f"  Sample Rate: {SAMPLE_RATE}\n")
        f.write(f"  Training Samples: {len(X_train):,}\n")
        f.write(f"  Validation Samples: {len(X_val):,}\n")
        f.write(f"  Features: {X.shape[1]}\n\n")
        
        for target, metrics in results.items():
            f.write(f"\n{target.replace('_', ' ').title()}:\n")
            f.write("-"*60 + "\n")
            for metric_name, value in metrics.items():
                f.write(f"  {metric_name:20s}: {value:.4f}\n")
    
    print(f"Saved evaluation metrics to {results_path}")
    
    # Final Summary
    print("\n" + "="*80)
    print("TRAINING SUMMARY")
    print("="*80)
    
    print(f"\nConfiguration:")
    print(f"  Model Type: {MODEL_TYPE.upper()}")
    print(f"  Lookback Window: {LOOKBACK_DAYS} days")
    print(f"  Training Samples: {len(X_train):,}")
    print(f"  Features: {X.shape[1]}")
    
    print("\n" + "="*80)
    print("PERFORMANCE RESULTS")
    print("="*80)
    
    print("\nRegression Models:")
    for target in ['sleep_quality', 'mood_score', 'stress_level']:
        r2 = results[target]['r2_score']
        rmse = results[target]['rmse']
        mae = results[target]['mae']
        
        # Assess performance
        if r2 >= 0.30:
            status = "âœ“ GOOD"
        elif r2 >= 0.20:
            status = "~ FAIR"
        else:
            status = "âœ— POOR"
        
        print(f"  {target:20s}: RÂ²={r2:6.4f} ({status}), RMSE={rmse:6.4f}, MAE={mae:6.4f}")
    
    print("\nClassification Model:")
    accuracy = results['exercise_binary']['accuracy']
    auc = results['exercise_binary']['auc_roc']
    f1 = results['exercise_binary']['f1_score']
    
    status = "âœ“ GOOD" if accuracy >= 0.70 else "~ FAIR" if accuracy >= 0.60 else "âœ— POOR"
    print(f"  {'exercise_binary':20s}: Acc={accuracy:.4f} ({status}), AUC={auc:.4f}, F1={f1:.4f}")
    
    # Compare with previous version
    print("\n" + "="*80)
    print("IMPROVEMENT vs V2")
    print("="*80)
    print("\nExpected improvements:")
    print("  Sleep Quality: 0.123 â†’ ~0.30-0.40 (RÂ²)")
    print("  Mood Score:    0.103 â†’ ~0.25-0.35 (RÂ²)")
    print("  Stress Level:  0.547 â†’ ~0.55-0.65 (RÂ²)")
    print("  Exercise:      0.671 â†’ ~0.72-0.78 (Accuracy)")
    
    print("\n" + "="*80)
    print("TRAINING COMPLETE!")
    print("="*80)
    
    print("\nFiles generated:")
    print(f"  ðŸ“Š Results: {results_dir}")
    print(f"  ðŸ’¾ Models:  {models_dir}")
    
    print("\nNext Steps:")
    print("  1. Review evaluation plots and metrics")
    print("  2. Check feature importance visualizations")
    print("  3. If performance is still low, try:")
    print("     - Increase sample_rate to 0.7-1.0")
    print("     - Add more lag features")
    print("     - Try model_type='xgboost' or 'lightgbm'")
    print("  4. Use trained models for predictions:")
    print("     from ml.models.health_predictor_v3 import ImprovedHealthPredictor")
    print("     predictor = ImprovedHealthPredictor()")
    print("     predictor.load_models()")
    print("     predictions = predictor.predict(X_new)")


if __name__ == "__main__":
    main()